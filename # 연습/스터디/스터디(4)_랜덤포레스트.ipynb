{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사결정나무와 랜덤포레스트( 예측 / 분류 모델 )\n",
    "## 13.3 의사결정나무와 랜덤포레스트( 예측 / 분류 모델 )\n",
    "\n",
    "> 뒤집어 놓은 나무와 같은 모양. 가지가 나눠지는 부분이 독립변수의 조건, 마지막 잎사귀들은 종속변수를 나타냄\n",
    "\n",
    "### 13.3.1 분류나무와 회귀나무\n",
    "- 의사결정나무는 명목형 종속변수를 분류할 수 있는 분류나무와 연속형 종속변수를 예측할 수 있는 회귀나무로 나뉜다.\n",
    "\n",
    "#### 분류나무\n",
    "- 분류나무는 기본적으로 데이터를 얼마나 잘 분류했는지 알 수 있는 척도인 불순도를 낮추고 순도를 높이는 방향으로 분류기준을 찾아낸다. 즉 한 노드내에서는 범주의 동질성이 높고, 노드 간에는 이질성이 최대한 높도록 만들어주는 것이다. \n",
    "\n",
    "- 데이터의 불순도를 나타내는 대표적인 기준은 지니계수와 엔트로\n",
    "- 분류를 통해 지니 계수나 엔트로피가 증가 혹은 감소한 양을 정보 획득량이라고 부른다. \n",
    "<20명에 대한 지니계수>\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "<45세 연령기준으로 첫번째 분류>\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "> 20명에 대한 지니계수에서 집단에 따른 지니계수 빼주면 된다. \n",
    "> 하지만 집단이 많을수록 지니계수가 너무 커지게 되므로 가중치를 부여해준다.\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "\n",
    "- 엔트로피는 지니 계수와 비슷하지만 이진 로그를 취함으로써 정규화 과정을 거치게 되고, 값의 범위는 0~1을 갖게된다. \n",
    "\n",
    "- 엔트로피 공식\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "- 엔트로피의 정보 획득량도 지니계수와 마찬가지로 데이터 비율의 가중치를 주어 구할 수 있다. \n",
    "\n",
    "- 분류나무는 각 독립변수의 수치마다 지니 계수나 엔트로피 값을 구해서 최적의 정보 획득량을 얻을 수 있는 기준으로 분류를 해 나가는 것\n",
    "- 1회 자식노드를 만들기 위해서 변수가 k개 , 관측치가 n개라고 했을 때 k(n-1)번 계산하고, 정보 획득량이 없을 때까지 가지를 뻗어나감. \n",
    "\n",
    "- 의자결정 나무의 종류\n",
    "\n",
    "![Alt text](image-7.png)\n",
    "\n",
    "#### 회귀나무\n",
    "\n",
    "- 회구나무는 종속변수가 연속형 변수이기 떄문에 잔차 제곱합 등의 분류 기준을 사용한다.\n",
    "- 회귀나무는 종속변수의 비선형성에 영향을 받지 않기 떄문에 일반 선형회귀분석에 비해 모델활용이 까다롭지않다.\n",
    "> 회귀나무에서 각 잎 노드에서 종속변수의 예측은 해당 노드 내의 데이터 포인트의 평균값으로 계산된다. 이로써 각 잎 노드마다 다른수준의 종속변수 값을 예측할 수 있으며 이는 비선형성을 잡아낼수 있는 특징임. 즉 각각의 잎 노드에서 종속변수의 다른 수준을 예측하기 때문에 선형적이라고 말할 수 없음.\n",
    "\n",
    "![Alt text](image-8.png)\n",
    "\n",
    "- 회귀나무는 끝 노드에 속한 데이터 값의 평균을 구해 회귀 예측값을 계산한다. --> 비선형적임.\n",
    "\n",
    "- 회귀나무는 그 밖에 F값이나 분산의 감소량을 분류 기준으로 사용한다.\n",
    "- F값이 크다는것 = 노드 간의 이질성이 높다는 것을 의미함. \n",
    "- 분산의 감소량의 경우, 한 노드에 속한 관측치 값들의 분산이 작아진다는 것은 동질성이 높다는 것을 의미한다. \n",
    "\n",
    "### 13.3.2 의사결정나무 모델의 장단점\n",
    "#### 장점\n",
    "- 의사결정나무 모델은 비선형 모델이기 때문에 회귀분석과 같이 데이터의 선형성, 정규성, 등분산성 등이 필요하지 않다. \n",
    "\n",
    "#### 단점\n",
    "- 의사결정나무와 일반 회귀 모델의 경우 명목형 변수는 예측 데이터에 있는 정보가 학습데이터에 없으면 예측이 불가능\n",
    "- 의사결정나무는 학습데이터에 있는 연속형 변수의 값만큼만 예측데이터에 적용되기 떄문에, 예측할 수 있는 종속변수의 최소 최댓값은 학습 데이터의 범위에 한정된다.\n",
    "\n",
    "> 따라서 학습데이터와 예측데이터의 연속형 변수값 편차가 큰경우에는 예측력이 떨어질 수 있음.\n",
    "\n",
    "- 학습 데이터에 과적합 될 확률이 높다. \n",
    "\n",
    "### 13.3.3 의사결정나무 모델의 과적합 방지를 위한 방법\n",
    "\n",
    "\n",
    "- 과적합을 방지하기 위해 가지치기, 정보획득량 임곗값 설정, 한 노드에 들어가는 최소 데이터 수 제한하기, 노드의 최대 깊이 제한하기 등이 있다. \n",
    "\n",
    "#### 가지치기\n",
    "- 가지치기 : 모델의 분기 가지들을 적절히 쳐내 과도하게 세밀하게 분기된 부분들을 없애준다. 이는 버리는것이 아니라, 분기를 합치는 개념.\n",
    "\n",
    "- 가지치기의 기준 : 분기 가지가 많아질수록 학습데이터의 오분류율은 낮아지게 되고, 특정수준 이상이 되면 검증 데이터의 오분류율은 높아지는 원리는 이용함.\n",
    "\n",
    "![Alt text](image-9.png)\n",
    "\n",
    "> 학습데이터로 만들어진 나무 모델응ㄹ 검증데이터에 반영하여, 오분류율을 확인한다. 끝단 가지들을 조금씩 쳐내가면서 검증데이터의 오분류율이 최소가 되는 시점까지 가지치기를 실시. --> 학습데이터의 모델정확도는 다소 떨어지더라도, 과적합을 방지하여 모델을 일반화할 수 있다.\n",
    "\n",
    "#### 정보 획득량 임곗값 설정\n",
    "- 분기를 했을 때 정보 획득량이 너무 적으면 분기를 멈추도록 설정하는것이다.\n",
    "- 한 노드에 들어가는 최소 데이터 수를 제한하는 방법과\n",
    "- 임계값은 노드를 분할하는 데 필요한 정보 획득량의 최소 기준을 정의 정보 획득량이 임곗값보다 낮은 분할은 수행되지 않고 노드의 분할이 중단\n",
    "\n",
    "#### 노드의 최대 깊이 제한\n",
    "- 노드의 최대깊이를 제한하는 방법 역시 가지가 너무 세세하게 분리되는것을 방지해 주기 떄문에 일반화된 의사결정나무 모델을 만들 떄 많이 쓰인다.\n",
    "- 노드의 최대 깊이를 제한하면 모델의 복잡성을 줄이고 과적합을 방지할 수 잇다. -> 특정 깊이 제한을 설정하면 트리가 특정 깊이에 도달할 때, 더이상 분할하지 않고 종료\n",
    "- 너무 작게 설정하면 과소적합일어남.\n",
    "\n",
    "420\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
