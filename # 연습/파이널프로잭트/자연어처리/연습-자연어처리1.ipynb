{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkYHk9zUdYJ"
      },
      "source": [
        "분포 가설이란 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다는 의미다. 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단하는 방법인데, 크게 두 가지 방법으로 나뉜다.\n",
        "\n",
        "특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 세는 방법인 카운트 기반 방법과 신경망 등을 통해 문맥 안의 단어 들을 예측하는 방법 인 예측으로 나뉜다.\n",
        "\n",
        "카운트 기반 방법 : 카운트 기반 방법은 기본적으로 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법을 사용하는 방식인데 다음과 같은 방법들이 있다.\n",
        "1. 특이값 분해\n",
        "2. 잠재의미분석\n",
        "3. Hyperspace Analogue Language(HAL)\n",
        "4. Hellinger PCA(Principal Component Analysis)\n",
        "\n",
        "위의 방법 들은 모두 동시 출현 행렬을 만들고 그 행렬들을 변형하는 방식인데 이 책에서는 동시 출현 행렬까지만 다룬다.\n",
        "\n",
        "카운트 기반 방법의 장점은 우선 빠르다. 예측 방법에 비해 더 과거에 나온 방법이지만 데이터가 많을 경우 단어가 잘 표현되고 효율적이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8vxNk5WEY3"
      },
      "source": [
        "예측 방법이란 신경망 구조 혹은 어떠한 모델을 사용해 특정 문맥에서 어떤 단어가 나올 지를 예측하면서 단어를 벡터로 만드는 방식이다.\n",
        "1. Word2vec\n",
        "2. NNLM(Neural Network Language Model)\n",
        "3. RNNLM(Recurrent Neural Network Language Model)\n",
        "\n",
        "Word2vec은 CBOW와 Skip-Gram이라는 두 가지 모델로 나뉜다. 두 모델은 서로 반대되는 개념으로 생각하면 되는데, CBOW의 경우 어떤 단어를 문맥 안의 주변 단어들을 통해 예측하는 방법이다. 반대로 Skip-Gram의 경우에는 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 방법이다.\n",
        "\n",
        "각 모델의 학습 방법에 대해 설명하면 CBOW의 경우 다음과 같은 순서로 학습한다.\n",
        "1. 각 주변 단어를 원-핫 벡터로 만들어 입력값으로 사용한다.\n",
        "2. 가중치 행렬을 각 원-핫 벡터에 곱해서 N-차원 벡터를 만든다.(은닉층)\n",
        "3. 만들어진 N-차원 벡터를 모두 더한 후 개수로 나눠 평균 N-차원 벡터를 만든다.(출력층 벡터)\n",
        "4. N-차원 벡터에서 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다.\n",
        "5. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다.\n",
        "\n",
        "Skip-Gram의 학습 방법도 비슷한 과정으로 진행한다. 전체 과정은 다음과 같다.\n",
        "1. 하나의 단어를 원-핫 벡터로 만들어서 입력값으로 사용한다.(입력층 벡터)\n",
        "2. 가중치 행렬을 원-핫 벡터에 곱해서 N-차원 벡터를 만든다(은닉층)\n",
        "3. n-차원 벡터에서 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다.(출력층 벡터)\n",
        "4. 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 원-핫 벡터와 비교해서 학습한다.\n",
        "\n",
        "CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교한다. Skip-Gram에서는 입력값이 하나의 단어를 사용하고, 학습을 위해 주변의 여러 단어와 비교한다. 위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용한다.\n",
        "\n",
        "-> 기존의 카운트 방법으로 만든 단어 벡터보다 단어 간의 유사도를 잘 측정한다. 또 한 가지 장점은 단어들의 복잡한 특징까지도 잘 잡아낸다는 점이다.\n",
        "\n",
        "-> CBOW와 Skip-Gram이 성능이 좋아 일반적인 경우 Skip-Gram을 사용한다. 하지만 절대적인 것은 아니니 두 가지 모두를 쓰는 것을 권장한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1qsfpAMZ316"
      },
      "source": [
        "텍스트 분류는 자연어 처리 문제 중 가장 대표적이고 많이 접하는 문제다.\n",
        "예시) 스팸 분류, 감정 분류, 뉴스 기사 분류, 지도 학습을 통한 텍스트 분류\n",
        "\n",
        "지도학습을 통한 텍스트 분류에는 다양한 모델이 있다.\n",
        "1. 나이브 베이즈 분류\n",
        "2. 서포트 벡터 분류\n",
        "3. 신경망\n",
        "4. 선형 분류\n",
        "5. 로지스틱 분류\n",
        "6. 랜덤 포레스트\n",
        "\n",
        "비지도 학습을 통한 텍스트 분류\n",
        "1. k-평균 군집화\n",
        "2. 계층적 군집화\n",
        "\n",
        "텍스트 유사도\n",
        "1. 자카드 유사도\n",
        "2. 유클리디언 유사도\n",
        "3. 맨해튼 유사도\n",
        "4. 코사인 유사도\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYyZqT-aazyx",
        "outputId": "9a38f65c-e5d0-46bc-bc67-ad9f3d9d7e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'갑작스런': 1.4054651081081644, '내일': 1.4054651081081644, '놀러왔다가': 1.4054651081081644, '망연자실': 1.4054651081081644, '반가운': 1.4054651081081644, '서쪽': 1.4054651081081644, '소식이': 1.4054651081081644, '오늘': 1.4054651081081644, '이어졌는데요': 1.4054651081081644, '인해': 1.4054651081081644, '있습니다': 1.0, '중심으로': 1.4054651081081644, '폭염': 1.0, '피해서': 1.4054651081081644, '하고': 1.4054651081081644, '휴일': 1.0}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "sent = (\"휴일 인 오늘 도 서쪽 을 중심으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식이 있습니다.\", \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.\")\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sent)\n",
        "idf = tfidf_vectorizer.idf_\n",
        "print(dict(zip(tfidf_vectorizer.get_feature_names_out(), idf)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m7odW5TBKoG"
      },
      "source": [
        "자카드 유사도 : 두 문장을 각각 단어의 집합으로 만든 뒤 두 지합을 통해 유사도를 측정하는 방식 중 하나다. 유사도를 측정하는 방법은 두 집합의 교집합인 공통의 단어의 개수를 두 집합의 합집합, 즉 전체 단어의 수로 나누면 된다. 1에 가까울수록 유사도가 높다는 뜻.\n",
        "\n",
        "자카드 공식에 위의 예제를 대입해 본다면 A,B의 교집합 개수는 6개, A와B의 합집합 개수는 24이므로 자카드 유사도는 6/24 = 0.25이다.\n",
        "\n",
        "코사인 유사도 : 두 개의 벡터값에서 코사인 각도를 구하는 방법이다. 코사인 유사도값은 -1에서 1 사이의 값을 가지고 1에 가까울수록 유사하다. 코사인 유사도는 유사도를 계산할 때 가장 널리 쓰이는 방법 중 하나다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ledbf9PF_82V",
        "outputId": "02a582dd-86fa-4464-dc35-a51d82d6cacd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.18976728]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix[1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68nt88ecENeu"
      },
      "source": [
        "유클리디언 유사도 : 가장 기본적인 거리를 측정하는 유사도 공식, 여기서 구하는 거리는 유클리디언 거리 혹은 L2거리라고 불리며, n차원 공간에서 두 점 사이의 최단 거리를 구하는 접근법이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4grm563hD_YW",
        "outputId": "840b0098-73ff-497c-e9bd-03e5f69136da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.27297503]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "euclidean_distances(tfidf_matrix[0:1],tfidf_matrix[1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "유클리디언 유사도는 단순히 두 점 사이의 거리를 뜻함. \n",
        "\n",
        "위의값을 0~1 사이의 값을 내도록 만들기 위해 앞서 각 문장을 백터화 했었는데 이벡터를 일반화 한 후 다시 유클리디언 유사도를 측정하면 0~1 사이의 값을 가지게 된다 . L1정규화 방법을 예로 들어보자 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.20881192]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np \n",
        "def l1_normalize(v):\n",
        "    norm = np.sum(v)\n",
        "    return v / norm\n",
        "\n",
        "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
        "euclidean_distances(tfidf_norm_l1[0:1],tfidf_norm_l1[1:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "맨해튼 유사도는 맨해튼 거리를 통해 유사도를 측정하는 방법 . 맨해튼 거리란 사각형 격자로 이뤄진 지도에서 출발점에서 도착점 까지를 가로지르지않고 살수있는 최적거리를 구하는 공식이다. L1거리라고도 부름\n",
        "\n",
        "이 방법 역시 L1정규화 방법을 사용해 백터안의 요소값을 정규화한뒤 유사도를 측정한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.76007301]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "manhattan_distances(tfidf_norm_l1[0:1],tfidf_norm_l1[1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 방법은 때로 값이 달라지기 때문에 문장 유사도 검사를 할 때에는 기본적으로 위의 방법을 모두 사용해보고 가장 성능이 좋은 방법을 택한다. 예제의 경우 맨해튼 거리를 선택한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
